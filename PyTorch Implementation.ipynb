{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch Implementation\n",
    "\n",
    "\n",
    "In this notebook it is explained the steps to follow on PyTorch.  \n",
    "Note that the real script is in the PyTorch folder. Therefore, in order for this to run, you need to take care of the dependencies references to be able to import them corretly\n",
    "\n",
    "\n",
    "### Import dependecies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.models as models\n",
    "\n",
    "from PyTorch.utils import image_loader, image_drawer\n",
    "from PyTorch.utils import gram_matrix, Normalization, get_input_optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuration\n",
    "\n",
    "We need to define the path to the images.  \n",
    "Remember from the documentation that we need images to replicate their content and style image to copy their artistic styles.  \n",
    "We also want to make a folder to store the outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Configuration Parameters '''\n",
    "cuda = torch.cuda.is_available()\n",
    "device = torch.device('cuda' if cuda else 'cpu')\n",
    "os.chdir('/Users/pabloruizruiz/OneDrive/Proyectos/AI/ML/COMPUTER VISION/Neural-Style-Transfer/PyTorch')\n",
    "\n",
    "#imsize = 512 if cuda else 128\n",
    "imsize = 512\n",
    "path_to_images = os.path.join(os.getcwd(), '../Images')\n",
    "path_to_content = os.path.join(path_to_images, 'content')\n",
    "path_to_style   = os.path.join(path_to_images, 'styles')\n",
    "path_to_outputs = os.path.join(path_to_images, 'pytorch_outputs')\n",
    "\n",
    "\n",
    "assert os.path.exists(path_to_images), 'Image folder does not exist'\n",
    "assert os.path.exists(path_to_content), 'Content images folder does not exist'\n",
    "assert os.path.exists(path_to_style), 'Style images folder does not exist'\n",
    "assert os.path.exists(path_to_outputs), 'Output folder does not exist'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 - Load images\n",
    "\n",
    "The starting point could be a noise image (like the shown in the documentation) or also the content image. So the content loss will be small and the style loss will be big at the beggining, until they get balanced depending on th weights we define for them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 - Load Images\n",
    "\n",
    "content_path = os.path.join(path_to_content, 'retiro.jpg')\n",
    "content_image = image_loader(content_path, imsize, device)\n",
    "\n",
    "style_path = os.path.join(path_to_style, 'fornite_map.jpg')\n",
    "style_image = image_loader(style_path, imsize, device)\n",
    "\n",
    "# To start with the content image\n",
    "input_image = content_image.clone()  \n",
    "# To start with a noise image\n",
    "# input_image = torch.randn(content_image.data.size(), device=device)\n",
    "\n",
    "# Sanity check\n",
    "assert content_image.size() == style_image.size(), \\\n",
    "    'Content and Image sizes must match'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the images\n",
    "plt.figure()\n",
    "image_drawer(content_image, title='Content Image')\n",
    "\n",
    "plt.figure()\n",
    "image_drawer(style_image, title='Style Image')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://github.com/PabloRR100/Neural-Style-Transfer/blob/master/Documentation/images/content_image.png?raw=true\" alt=\"content\" style=\"width:60%\"/>\n",
    "\n",
    "<img src=\"https://github.com/PabloRR100/Neural-Style-Transfer/blob/master/Documentation/images/style_image.png?raw=true\" alt=\"style\" style=\"width:60%\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 - Define the losses\n",
    "\n",
    "The Content Loss is simply the distance of the image with respect to the target content image.  \n",
    "\n",
    "The Style Loss is the weighted average distance of the Gram Matrix of the feature maps at different layers of the network with the Gram Matrix of the style target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2 - Content Loss\n",
    "class ContentLoss(nn.Module):\n",
    "\n",
    "    def __init__(self, target):\n",
    "        super(ContentLoss, self).__init__()\n",
    "        self.target = target.detach()\n",
    "\n",
    "    def forward(self, input):\n",
    "        self.loss = F.mse_loss(input, self.target)\n",
    "        return input\n",
    "\n",
    "# 3 - Style Loss\n",
    "class StyleLoss(nn.Module):\n",
    "\n",
    "    def __init__(self, target_feature):\n",
    "        super(StyleLoss, self).__init__()\n",
    "        self.target = gram_matrix(target_feature).detach()\n",
    "\n",
    "    def forward(self, input):\n",
    "        G = gram_matrix(input)\n",
    "        self.loss = F.mse_loss(G, self.target)\n",
    "        return input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4 - Define the layers where we will extract the feature maps\n",
    "\n",
    "As we said, we will average the feature maps from different convolutional layers of the CNN for the style loss.  \n",
    "\n",
    "But also, for the content we will get the feature maps at some layer. As you could see in the documentation, we get the content from some late convolution layer, as we are interested in keeping **only the most important feature** and leave some space to capture the style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4 - Add loss modules in the right order to modules from pretrained model\n",
    "\n",
    "content_layers = ['conv_4']\n",
    "style_layers = ['conv_1', 'conv_2', 'conv_3', 'conv_4', 'conv_5']\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5 - Load a pretrained Network\n",
    "\n",
    "In Neural Style Transfer, the weights of the neural network remains constant.  \n",
    "This is because the gradients are computed respect to the input pixels, not any wieghts.  \n",
    "Therefore, it makes sense to make use of a powerful already pre-trained model like VGG19, in this particular case:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5 - Load Pre-trained Network\n",
    "''' For style tranfer VGG is the best architecture '''\n",
    "\n",
    "cnn = models.vgg19(pretrained=True).features.to(device).eval()\n",
    "\n",
    "cnn_normalization_mean = torch.tensor([0.485, 0.456, 0.406]).to(device)\n",
    "cnn_normalization_std = torch.tensor([0.229, 0.224, 0.225]).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are keeping only the Sequential module ```features``` as we are only interested in this part of the VGG - we are getting rid of the classifier block.  \n",
    "\n",
    "Also, as we are not training the network, we set in into evaluation mode with ```.eval()```\n",
    "\n",
    "Furthermore, as VGG is trained on images with the channels normalized, we used the values used on that training to normlize our images before feeding them to the VGG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Construct the model from this pretrained model\n",
    "\n",
    "Now we are just going to replicate the architecture of the pretrained Sequential modules of the VGG, and append our defined losses on the layer we chose above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create the desired architecture from the pretrained model\n",
    "def get_style_model_and_losses(cnn, normalization_mean, normalization_std,\n",
    "                               style_img, content_img,\n",
    "                               content_layers = content_layers,\n",
    "                               style_layers = style_layers):\n",
    "\n",
    "    cnn = copy.deepcopy(cnn)\n",
    "    normalization = Normalization(normalization_mean, normalization_std).to(device)\n",
    "\n",
    "    content_losses = []\n",
    "    style_losses = []\n",
    "\n",
    "    model = nn.Sequential(normalization)\n",
    "    i = 0  # Control variable to count Conv layers passed\n",
    "    for layer in cnn.children():\n",
    "        if isinstance(layer, nn.Conv2d):\n",
    "            i += 1\n",
    "            name = 'conv_{}'.format(i)\n",
    "        elif isinstance(layer, nn.ReLU):\n",
    "            name = 'relu_{}'.format(i)\n",
    "            layer = nn.ReLU(inplace=False)\n",
    "        elif isinstance(layer, nn.MaxPool2d):\n",
    "            name = 'pool_{}'.format(i)\n",
    "        elif isinstance(layer, nn.BatchNorm2d):\n",
    "            name = 'bn_{}'.format(i)\n",
    "        else:\n",
    "            raise RuntimeError('Unrecognized layer: {}'.format(layer.__class__.__name__))\n",
    "\n",
    "        model.add_module(name, layer)\n",
    "\n",
    "        # Add Content Loss\n",
    "        if name in content_layers:\n",
    "            target = model(content_img).detach()\n",
    "            content_loss = ContentLoss(target)\n",
    "            model.add_module(\"content_loss_{}\".format(i), content_loss)\n",
    "            content_losses.append(content_loss)\n",
    "\n",
    "        # Add Style Loss\n",
    "        if name in style_layers:\n",
    "            target_feature = model(style_img).detach()\n",
    "            style_loss = StyleLoss(target_feature)\n",
    "            model.add_module(\"style_loss_{}\".format(i), style_loss)\n",
    "            style_losses.append(style_loss)\n",
    "\n",
    "    # now we trim off the layers after the last content and style losses\n",
    "    for i in range(len(model) - 1, -1, -1):\n",
    "        if isinstance(model[i], ContentLoss) or isinstance(model[i], StyleLoss):\n",
    "            break\n",
    "\n",
    "    model = model[:(i + 1)]\n",
    "\n",
    "    return model, style_losses, content_losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6 - Training Algorithm\n",
    "\n",
    "The original paper suggest to use ```L-BFGS```algorithm to run the gradient descense optimization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6 - Training Algorithm\n",
    "def run_style_transfer(cnn, normalization_mean, normalization_std,\n",
    "                       content_img, style_img, input_img, num_steps=2000,\n",
    "                       style_weight=1000000, content_weight=10):\n",
    "    print('Building model...')\n",
    "    model, style_losses, content_losses = get_style_model_and_losses(cnn,\n",
    "        normalization_mean, normalization_std, style_img, content_img)\n",
    "    optimizer = get_input_optimizer(input_img)\n",
    "\n",
    "    run = [0]\n",
    "    while run[0] <= num_steps:\n",
    "\n",
    "        def closure():\n",
    "            # correct the values of updated input image\n",
    "            input_img.data.clamp_(0, 1)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            model(input_img)\n",
    "            style_score = 0\n",
    "            content_score = 0\n",
    "\n",
    "            for sl in style_losses:\n",
    "                style_score += sl.loss\n",
    "            for cl in content_losses:\n",
    "                content_score += cl.loss\n",
    "\n",
    "            style_score *= style_weight\n",
    "            content_score *= content_weight\n",
    "\n",
    "            loss = style_score + content_score\n",
    "            loss.backward()\n",
    "\n",
    "            run[0] += 1\n",
    "            if run[0] % 50 == 0:\n",
    "                print(\"run {}:\".format(run))\n",
    "                print('Style Loss : {:4f} Content Loss: {:4f}'.format(\n",
    "                    style_score.item(), content_score.item()))\n",
    "                print()\n",
    "\n",
    "            return style_score + content_score\n",
    "\n",
    "        optimizer.step(closure)\n",
    "\n",
    "    # Clip to ensure the values makes sense and are between [0-1]\n",
    "    input_img.data.clamp_(0, 1)\n",
    "    return input_img\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7 - Run and fun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = run_style_transfer(cnn, cnn_normalization_mean, cnn_normalization_std,\n",
    "                            content_image, style_image, input_image)\n",
    "\n",
    "plt.figure()\n",
    "image_drawer(output, title='Output Image')\n",
    "plt.savefig(os.path.join(path_to_outputs, 'Retiro2'))\n",
    "plt.ioff()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://github.com/PabloRR100/Neural-Style-Transfer/blob/master/Documentation/images/retiro_fornite.png?raw=true\" alt=\"content\" style=\"width:60%\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
